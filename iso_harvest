#! /usr/bin/python

import os
import sys
import requests
from thredds_crawler.crawl import Crawl

import logging
import logging.handlers

logger = logging.getLogger('thredds_crawler')
fh = logging.handlers.RotatingFileHandler('/var/log/iso_harvest/iso_harvest.log', maxBytes=1024*1024*10, backupCount=5)
fh.setLevel(logging.DEBUG)
ch = logging.StreamHandler()
ch.setLevel(logging.DEBUG)
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
fh.setFormatter(formatter)
ch.setFormatter(formatter)
logger.addHandler(fh)
logger.addHandler(ch)
logger.setLevel(logging.DEBUG)

SAVE_DIR = "/srv/http/iso"


def harvest_isos(catalogUrl, outDir, select=None, skip=None):
    rootpath = os.path.join(SAVE_DIR, outDir)
    foundisos = []
    catalog = Crawl(catalogUrl, select=select, skip=skip, debug=True)
    isos = [(d.id, s.get("url")) for d in catalog.datasets for s in d.services if s.get("service").lower() == "iso"]
    for iso in isos:
        try:
            filename = iso[0].replace("/", "_") + ".iso.xml"
            foundisos.append(filename)
            filepath = os.path.join(rootpath, filename)
            logger.info("Downloading/Saving %s" % filepath)

            r = requests.get(iso[1], stream=True)
            if r.ok:
                with open(filepath, 'wb') as f:
                    for chunk in r.iter_content():
                        if chunk:
                            f.write(chunk)
            else:
                logger.info("Got a non-200 status code (%s) from %s" % (r.status_code, iso[1]))
        except KeyboardInterrupt:
            logger.info("Caught interrupt, exiting")
            sys.exit(0)
        except BaseException:
            logger.exception("Error!")
    clean_not_found_files(rootpath, foundisos)


def clean_not_found_files(check_dir, foundfiles):
    for f in os.listdir(check_dir):
        if (f not in foundfiles):
            logger.info("%s not found in catalog, deleting...", f)
        os.remove(os.path.join(check_dir, f))

# AOOS
harvest_isos("http://thredds.axiomalaska.com/thredds/catalogs/aoos.html", "aoos")

# CeNCOOS
harvest_isos("http://thredds.axiomalaska.com/thredds/catalogs/cencoos.html", "cencoos")

# UNIDATA
unidata_selects = [".*Best.*"]
unidata_skips   = Crawl.SKIPS  + [".*grib2", ".*grib1", ".*GrbF.*", ".*ncx2",
                                  "Radar Data", "Station Data",
                                  "Point Feature Collections", "Satellite Data",
                                  "Unidata NEXRAD Composites \(GINI\)",
                                  "Unidata case studies"]
harvest_isos("http://thredds.ucar.edu/thredds/catalog.html", "unidata", select=unidata_selects, skip=unidata_skips)
